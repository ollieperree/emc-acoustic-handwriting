{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram-based resnet digit classifier using fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from librosa import display\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data directory and recordings directory\n",
    "DATA = Path(\"../../data/words/\")\n",
    "RECORDINGS = DATA/\"audio-recordings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(filename):\n",
    "    return librosa.core.load(filename, sr=None, mono=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(DATA/\"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_spectrogram(wav_path, save_path):\n",
    "    plt.clf()\n",
    "    data, sr = load_wav(wav_path)\n",
    "    trimmed, idx = librosa.effects.trim(data, top_db=30)\n",
    "    spec = librosa.feature.melspectrogram(trimmed, sr, n_fft=2048, hop_length=256)\n",
    "    librosa.display.specshow(librosa.core.power_to_db(spec))\n",
    "    cur_axes = plt.gca().set_axis_off()\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fname):\n",
    "    save_spectrogram(DATA/\"processed\"/fname, DATA/\"spectrograms\"/fname.replace(\".wav\", \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../../data/words/spectrograms/*.png\n",
    "\n",
    "if not (DATA/\"spectrograms\").exists():\n",
    "    (DATA/\"spectrograms\").mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8a7f892bd74b3b99e7bb6fd369f71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(4)  # Use 4 processes\n",
    "\n",
    "for fname in tqdm(labels_df[\"filename\"]):\n",
    "    pool.apply(process_file, args=(fname,))\n",
    "    \n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai import metrics\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe which has filenames ending in .png instead of .wav\n",
    "spec_labels_df = labels_df.copy()\n",
    "spec_labels_df[\"filename\"] = spec_labels_df[\"filename\"].apply(lambda x: x.replace(\".wav\", \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(valid_split_no, bs=32):\n",
    "    df = spec_labels_df[spec_labels_df[f\"valid{valid_split_no}\"] != -1]\n",
    "    return (ImageList.from_df(df, DATA/\"spectrograms\")\n",
    "            .split_from_df(col=f\"valid{valid_split_no}\")\n",
    "            .label_from_df(cols=\"label\")\n",
    "            .databunch(bs=bs)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SPLITS = [1, 2, 3]\n",
    "MODELS = [\n",
    "    #models.resnet18,\n",
    "    models.resnet34,\n",
    "    models.resnet50,\n",
    "    #models.resnet101,\n",
    "    models.densenet121,\n",
    "    models.densenet161\n",
    "]\n",
    "PRETRAINED = [True, False]\n",
    "USE_MIXUP = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "SPLIT 1\n",
      "************************************\n",
      "model=resnet18, pretrained=True, mixup=True\n",
      "epoch     train_loss  valid_loss  accuracy  precision  recall    time    \n",
      "0         5.798199    4.659916    0.037500  nan        0.037500  00:08     \n",
      "1         5.237535    4.463215    0.043056  nan        0.043056  00:07     \n",
      "2         4.824464    4.332203    0.048611  nan        0.048611  00:07     \n",
      "3         4.553841    4.287099    0.054167  nan        0.054167  00:07     \n",
      "4         4.284824    4.237043    0.059722  0.054026   0.059722  00:07     \n",
      "5         4.100189    4.131279    0.066667  nan        0.066667  00:07     \n",
      "6         3.901414    4.159503    0.066667  nan        0.066667  00:07     \n",
      "7         3.765960    4.139883    0.073611  nan        0.073611  00:07     \n",
      "8         3.701023    4.146627    0.090278  nan        0.090278  00:07     \n",
      "9         3.655941    4.090623    0.080556  nan        0.080556  00:07     \n",
      "10        3.626409    4.118668    0.076389  nan        0.076389  00:07     \n",
      "11        3.616857    4.064160    0.079167  nan        0.079167  00:07     \n",
      "12        3.629252    4.064701    0.093056  nan        0.093056  00:07     \n",
      "13        3.613549    3.872582    0.112500  nan        0.112500  00:07     \n",
      "14        3.525990    3.961070    0.115278  nan        0.115278  00:07     \n",
      "15        3.445156    3.811784    0.112500  nan        0.112500  00:07     \n",
      "16        3.346970    3.766866    0.118056  nan        0.118056  00:07     \n",
      "17        3.343983    3.569627    0.141667  nan        0.141667  00:07     \n",
      "18        3.296405    3.716310    0.122222  nan        0.122222  00:07     \n",
      "19        3.225072    3.569852    0.147222  nan        0.147222  00:07     \n",
      "20        3.197988    3.589131    0.147222  nan        0.147222  00:07     \n",
      "21        3.167469    3.778289    0.141667  nan        0.141667  00:07     \n",
      "22        3.158907    3.566858    0.154167  nan        0.154167  00:07     \n",
      "23        3.057788    3.504216    0.147222  nan        0.147222  00:07     \n",
      "24        3.018769    3.524900    0.152778  nan        0.152778  00:07     \n",
      "25        3.002599    3.826638    0.161111  nan        0.161111  00:07     \n",
      "26        2.980677    3.485668    0.181944  nan        0.181944  00:07     \n",
      "27        2.923872    3.436912    0.184722  nan        0.184722  00:07     \n",
      "28        2.891062    3.476410    0.151389  nan        0.151389  00:07     \n",
      "29        2.854446    3.345413    0.177778  nan        0.177778  00:07     \n",
      "30        2.843884    3.308316    0.213889  nan        0.213889  00:07     \n",
      "31        2.776965    3.354504    0.193056  nan        0.193056  00:07     \n",
      "32        2.723772    3.309776    0.200000  nan        0.200000  00:07     \n",
      "33        2.680848    3.450331    0.186111  nan        0.186111  00:07     \n",
      "34        2.637815    3.392447    0.204167  nan        0.204167  00:07     \n",
      "35        2.637148    3.279433    0.177778  nan        0.177778  00:07     \n",
      "36        2.605032    3.408356    0.200000  nan        0.200000  00:07     \n",
      "37        2.490907    3.416011    0.201389  nan        0.201389  00:07     \n",
      "38        2.495612    3.260142    0.219444  nan        0.219444  00:07     \n",
      "39        2.490697    3.159122    0.238889  nan        0.238889  00:07     \n",
      "40        2.429072    3.435920    0.200000  nan        0.200000  00:07     \n",
      "41        2.393231    3.390496    0.211111  nan        0.211111  00:07     \n",
      "42        2.384577    3.338107    0.220833  nan        0.220833  00:07     \n",
      "43        2.365098    3.245054    0.229167  0.291534   0.229167  00:07     \n",
      "44        2.294533    3.373535    0.208333  nan        0.208333  00:07     \n",
      "45        2.283657    3.420841    0.236111  nan        0.236111  00:07     \n",
      "46        2.241590    3.416157    0.223611  nan        0.223611  00:07     \n",
      "47        2.183175    3.454494    0.241667  nan        0.241667  00:07     \n",
      "48        2.164921    3.300238    0.229167  nan        0.229167  00:07     \n",
      "49        2.144157    3.401864    0.237500  0.338235   0.237500  00:07     \n",
      "50        2.131580    3.296656    0.241667  nan        0.241667  00:07     \n",
      "51        2.097615    3.444233    0.213889  nan        0.213889  00:07     \n",
      "52        2.077060    3.192797    0.250000  nan        0.250000  00:07     \n",
      "53        2.004977    3.374047    0.222222  nan        0.222222  00:07     \n",
      "54        2.007847    3.457447    0.237500  nan        0.237500  00:07     \n",
      "55        1.993194    3.379442    0.247222  nan        0.247222  00:07     \n",
      "56        1.891859    3.413912    0.245833  nan        0.245833  00:07     \n",
      "57        1.949243    3.352364    0.241667  nan        0.241667  00:07     \n",
      "58        1.876964    3.170780    0.227778  0.267419   0.227778  00:07     \n",
      "59        1.836502    3.197689    0.273611  nan        0.273611  00:07     \n",
      "60        1.854458    3.315707    0.244444  0.281287   0.244444  00:07     \n",
      "61        1.805313    3.230628    0.273611  nan        0.273611  00:07     \n",
      "62        1.761858    3.298010    0.250000  nan        0.250000  00:07     \n",
      "63        1.737058    3.249877    0.258333  0.322213   0.258333  00:07     \n",
      "64        1.786459    3.206573    0.270833  0.300445   0.270833  00:07     \n",
      "65        1.771719    3.328007    0.236111  0.321153   0.236111  00:07     \n",
      "66        1.723782    3.163173    0.283333  0.326363   0.283333  00:07     \n",
      "67        1.684454    3.152693    0.275000  nan        0.275000  00:07     \n",
      "68        1.665891    3.248825    0.243056  0.270165   0.243056  00:07     \n",
      "69        1.630780    3.185058    0.268056  nan        0.268056  00:07     \n",
      "70        1.623285    3.235847    0.256944  0.295804   0.256944  00:07     \n",
      "71        1.617446    3.154088    0.265278  0.303296   0.265278  00:07     \n",
      "72        1.634858    3.194545    0.265278  0.315230   0.265278  00:07     \n",
      "73        1.591563    3.147683    0.270833  0.300642   0.270833  00:07     \n",
      "74        1.565804    3.184036    0.272222  0.310263   0.272222  00:07     \n",
      "75        1.566409    3.156385    0.287500  nan        0.287500  00:07     \n",
      "76        1.560680    3.120890    0.276389  nan        0.276389  00:07     \n",
      "77        1.545075    3.153744    0.277778  0.309479   0.277778  00:07     \n",
      "78        1.556812    3.089813    0.270833  0.311876   0.270833  00:07     \n",
      "79        1.524162    3.059112    0.283333  0.302626   0.283333  00:07     \n",
      "80        1.514847    3.084001    0.283333  0.309730   0.283333  00:07     \n",
      "81        1.540376    3.089290    0.295833  0.331203   0.295833  00:07     \n",
      "82        1.507757    3.079093    0.290278  0.322908   0.290278  00:07     \n",
      "83        1.501635    3.059248    0.294444  0.310454   0.294445  00:07     \n",
      "84        1.454464    3.089185    0.288889  0.318530   0.288889  00:07     \n",
      "85        1.503860    3.065695    0.298611  0.324462   0.298611  00:07     \n",
      "86        1.501632    3.036739    0.294444  0.320973   0.294444  00:07     \n",
      "87        1.458286    3.057705    0.287500  0.309023   0.287500  00:07     \n",
      "88        1.466342    3.044037    0.291667  0.313253   0.291667  00:07     \n",
      "89        1.453227    3.049138    0.287500  0.310440   0.287500  00:07     \n",
      "90        1.462493    3.018920    0.301389  0.327493   0.301389  00:07     \n",
      "91        1.461374    3.008506    0.293056  0.313412   0.293056  00:07     \n",
      "92        1.453264    3.026484    0.288889  0.315646   0.288889  00:07     \n",
      "93        1.413124    3.008884    0.304167  0.324247   0.304167  00:07     \n",
      "94        1.453405    3.029217    0.294444  0.322515   0.294444  00:07     \n",
      "95        1.422830    3.017169    0.293056  0.312068   0.293056  00:07     \n",
      "96        1.430599    3.032740    0.298611  0.327632   0.298611  00:07     \n",
      "97        1.431630    3.042800    0.291667  0.320229   0.291667  00:07     \n",
      "98        1.441154    3.023102    0.293056  0.313065   0.293056  00:07     \n",
      "99        1.438112    3.037407    0.293056  0.316029   0.293056  00:07     \n",
      "model=resnet18, pretrained=True, mixup=False\n",
      "epoch     train_loss  valid_loss  accuracy  precision  recall    time    \n",
      "0         5.786185    4.539937    0.047222  0.036475   0.047222  00:07     \n",
      "1         4.857808    4.356331    0.059722  0.052670   0.059722  00:07     \n",
      "2         4.207271    4.294610    0.058333  nan        0.058333  00:07     \n",
      "3         3.742194    4.227184    0.055556  0.045387   0.055556  00:07     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4         3.286608    4.225522    0.066667  nan        0.066667  00:07     \n",
      "5         2.992630    4.241886    0.063889  nan        0.063889  00:07     \n",
      "6         2.736857    4.388347    0.050000  0.037545   0.050000  00:07     \n",
      "7         2.562820    4.498033    0.054167  nan        0.054167  00:07     \n",
      "8         2.530335    4.593337    0.080556  nan        0.080556  00:07     \n",
      "9         2.589300    4.797667    0.061111  nan        0.061111  00:07     \n",
      "10        2.598200    4.642245    0.061111  0.061270   0.061111  00:07     \n",
      "11        2.657790    4.781180    0.068056  nan        0.068056  00:07     \n",
      "12        2.821449    4.938893    0.075000  nan        0.075000  00:07     \n",
      "13        2.890401    4.774429    0.076389  nan        0.076389  00:07     \n",
      "14        2.841595    5.187504    0.070833  nan        0.070833  00:07     \n",
      "15        2.943459    4.604561    0.084722  nan        0.084722  00:07     \n",
      "16        2.922383    4.599244    0.094444  nan        0.094444  00:07     \n",
      "17        2.849049    4.398390    0.097222  nan        0.097222  00:07     \n",
      "18        2.811010    4.507552    0.098611  nan        0.098611  00:07     \n",
      "19        2.698770    4.069531    0.111111  nan        0.111111  00:07     \n",
      "20        2.642997    4.353031    0.105556  nan        0.105556  00:07     \n",
      "21        2.586623    4.218458    0.105556  nan        0.105556  00:07     \n",
      "22        2.506494    4.230905    0.095833  nan        0.095833  00:07     \n",
      "23        2.469217    4.183030    0.127778  nan        0.127778  00:07     \n",
      "24        2.433174    4.192960    0.095833  nan        0.095833  00:07     \n",
      "25        2.350108    4.282831    0.123611  nan        0.123611  00:07     \n",
      "26        2.260972    4.533086    0.111111  nan        0.111111  00:07     \n",
      "27        2.179207    3.917662    0.144444  nan        0.144444  00:07     \n",
      "28        2.153385    4.048357    0.154167  nan        0.154167  00:07     \n",
      "29        1.999826    4.110534    0.169444  nan        0.169444  00:07     \n",
      "30        1.945583    4.242578    0.163889  nan        0.163889  00:07     \n",
      "31        1.837728    4.511425    0.133333  nan        0.133333  00:07     \n",
      "32        1.750876    4.613354    0.137500  nan        0.137500  00:07     \n",
      "33        1.706588    4.387933    0.148611  nan        0.148611  00:07     \n",
      "34        1.611470    4.164656    0.156944  nan        0.156944  00:07     \n",
      "35        1.525018    4.398018    0.161111  nan        0.161111  00:07     \n",
      "36        1.450792    4.932992    0.141667  nan        0.141667  00:07     \n",
      "37        1.393085    4.754539    0.161111  nan        0.161111  00:07     \n",
      "38        1.336050    4.846000    0.156944  nan        0.156944  00:07     \n",
      "39        1.260610    5.147469    0.137500  nan        0.137500  00:07     \n",
      "40        1.243247    4.563756    0.159722  nan        0.159722  00:07     \n",
      "41        1.224834    5.000203    0.176389  nan        0.176389  00:07     \n",
      "42        1.141254    4.892601    0.165278  nan        0.165278  00:07     \n",
      "43        1.081802    5.413064    0.151389  nan        0.151389  00:07     \n",
      "44        0.997155    4.816207    0.186111  nan        0.186111  00:07     \n",
      "45        1.008587    5.198502    0.158333  nan        0.158333  00:07     \n",
      "46        0.980640    5.470011    0.141667  nan        0.141667  00:07     \n",
      "47        0.931310    4.753673    0.169444  nan        0.169444  00:07     \n",
      "48        0.867882    4.823098    0.177778  nan        0.177778  00:07     \n",
      "49        0.810630    5.037663    0.173611  nan        0.173611  00:07     \n",
      "50        0.824863    5.154017    0.181944  nan        0.181944  00:07     \n",
      "51        0.735361    5.820443    0.170833  nan        0.170833  00:07     \n",
      "52        0.762091    4.854558    0.190278  nan        0.190278  00:07     \n",
      "53        0.690597    5.211759    0.180556  nan        0.180556  00:07     \n",
      "54        0.620084    5.804625    0.168056  0.245027   0.168056  00:07     \n",
      "55        0.622392    5.750877    0.181944  nan        0.181944  00:07     \n",
      "56        0.622307    5.116536    0.201389  0.221968   0.201389  00:07     \n",
      "57        0.586561    5.190176    0.181944  nan        0.181944  00:07     \n",
      "58        0.463859    5.214507    0.218056  0.222192   0.218056  00:07     \n",
      "59        0.494451    5.264541    0.200000  0.211759   0.200000  00:07     \n",
      "60        0.496535    5.401008    0.202778  nan        0.202778  00:07     \n",
      "61        0.474832    5.616789    0.202778  0.243602   0.202778  00:07     \n",
      "62        0.437659    5.299636    0.198611  nan        0.198611  00:07     \n",
      "63        0.392436    5.409052    0.201389  nan        0.201389  00:07     \n",
      "64        0.393637    5.961011    0.205556  nan        0.205556  00:07     \n",
      "65        0.362134    5.644373    0.205556  0.214463   0.205556  00:07     \n",
      "66        0.332989    5.411098    0.202778  nan        0.202778  00:07     \n",
      "67        0.320424    5.367717    0.218056  0.223984   0.218056  00:07     \n",
      "68        0.265308    5.226881    0.230556  0.237061   0.230556  00:07     \n",
      "69        0.267639    5.409540    0.218056  0.238947   0.218056  00:07     \n",
      "70        0.243841    5.471146    0.215278  0.226486   0.215278  00:07     \n",
      "71        0.245201    5.303923    0.204167  0.221439   0.204167  00:07     \n",
      "72        0.215699    5.265130    0.223611  0.213871   0.223611  00:07     \n",
      "73        0.220330    5.333426    0.218056  0.220612   0.218056  00:07     \n",
      "74        0.184682    5.533081    0.212500  0.214575   0.212500  00:07     \n",
      "75        0.178843    5.429444    0.222222  0.213070   0.222222  00:07     \n",
      "76        0.164629    5.517688    0.223611  0.234605   0.223611  00:07     \n",
      "77        0.148677    5.507515    0.222222  0.225417   0.222222  00:07     \n",
      "78        0.131728    5.455996    0.230556  0.232527   0.230556  00:07     \n",
      "79        0.125975    5.495362    0.238889  0.251342   0.238889  00:07     \n",
      "80        0.120546    5.434975    0.234722  0.239293   0.234722  00:07     \n",
      "81        0.105012    5.424094    0.248611  0.247930   0.248611  00:07     \n",
      "82        0.107873    5.408876    0.251389  0.266295   0.251389  00:07     \n",
      "83        0.094227    5.444881    0.240278  0.261234   0.240278  00:07     \n",
      "84        0.084368    5.515471    0.236111  0.254300   0.236111  00:07     \n",
      "85        0.087343    5.489200    0.226389  0.227150   0.226389  00:07     \n",
      "86        0.090693    5.542222    0.237500  0.245232   0.237500  00:07     \n",
      "87        0.084485    5.493075    0.243056  0.244466   0.243056  00:07     \n",
      "88        0.068231    5.560217    0.244444  0.252764   0.244444  00:07     \n",
      "89        0.073907    5.565153    0.238889  0.248347   0.238889  00:07     \n",
      "90        0.066466    5.516262    0.243056  0.234356   0.243056  00:07     \n",
      "91        0.063302    5.502123    0.244444  0.247913   0.244444  00:07     \n",
      "92        0.056865    5.546708    0.241667  0.243881   0.241667  00:07     \n",
      "93        0.059659    5.575184    0.247222  0.250398   0.247222  00:07     \n",
      "94        0.051621    5.510012    0.245833  0.245707   0.245833  00:07     \n",
      "95        0.056756    5.603710    0.243056  0.247067   0.243056  00:07     \n",
      "96        0.068778    5.581288    0.237500  0.242049   0.237500  00:07     \n",
      "97        0.051758    5.490169    0.245833  0.240207   0.245833  00:07     \n",
      "98        0.056418    5.503822    0.237500  0.232690   0.237500  00:07     \n",
      "99        0.052846    5.484785    0.236111  0.233035   0.236111  00:07     \n",
      "model=resnet18, pretrained=False, mixup=True\n",
      "epoch     train_loss  valid_loss  accuracy  precision  recall    time    \n",
      "0         5.462898    5.542345    0.026389  nan        0.026389  00:09     \n",
      "1         5.069745    4.492955    0.041667  nan        0.041667  00:09     \n",
      "2         4.856050    4.908280    0.034722  nan        0.034722  00:09     \n",
      "3         4.771342    4.414292    0.054167  nan        0.054167  00:09     \n",
      "4         4.638330    6.263319    0.055556  nan        0.055556  00:09     \n",
      "5         4.631100    8.887145    0.026389  nan        0.026389  00:09     \n",
      "6         4.554992    16.796564   0.023611  nan        0.023611  00:09     \n",
      "7         4.472928    3.945215    0.050000  nan        0.050000  00:09     \n",
      "8         4.336721    6.741174    0.069444  nan        0.069444  00:09     \n",
      "9         4.303680    6.047083    0.058333  nan        0.058333  00:09     \n",
      "10        4.290296    66.773842   0.012500  nan        0.012500  00:09     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11        4.282626    3.988843    0.075000  nan        0.075000  00:09     \n",
      "12        4.027787    4.850452    0.062500  nan        0.062500  00:09     \n",
      "13        3.897066    4.012793    0.076389  nan        0.076389  00:09     \n",
      "14        3.754148    3.630597    0.118056  nan        0.118056  00:09     \n",
      "15        3.581100    5.621696    0.044444  nan        0.044444  00:09     \n",
      "16        3.364202    3.669604    0.131944  nan        0.131944  00:09     \n",
      "17        3.284334    3.496647    0.145833  nan        0.145833  00:09     \n",
      "18        3.149924    3.475070    0.140278  nan        0.140278  00:09     \n",
      "19        3.080962    6.390936    0.029167  nan        0.029167  00:09     \n",
      "20        2.980977    3.633442    0.145833  nan        0.145833  00:09     \n",
      "21        2.888712    3.327877    0.179167  nan        0.179167  00:09     \n",
      "22        2.818577    3.650597    0.163889  nan        0.163889  00:09     \n",
      "23        2.668182    3.098652    0.211111  nan        0.211111  00:09     \n",
      "24        2.585739    3.319810    0.211111  nan        0.211111  00:09     \n",
      "25        2.521780    3.312242    0.222222  nan        0.222222  00:09     \n",
      "26        2.449169    3.830212    0.166667  nan        0.166667  00:09     \n",
      "27        2.334207    3.007920    0.261111  nan        0.261111  00:09     \n",
      "28        2.264356    3.655411    0.181944  nan        0.181944  00:09     \n",
      "29        2.194644    3.308598    0.240278  nan        0.240278  00:09     \n",
      "30        2.153954    4.097099    0.161111  nan        0.161111  00:09     \n",
      "31        2.092535    3.555316    0.201389  nan        0.201389  00:09     \n",
      "32        2.040310    3.142554    0.258333  nan        0.258333  00:09     \n",
      "33        1.989306    3.210322    0.261111  nan        0.261111  00:09     \n",
      "34        1.883858    3.190104    0.266667  nan        0.266667  00:09     \n",
      "35        1.794878    3.223189    0.231944  nan        0.231944  00:09     \n",
      "36        1.767687    3.441034    0.226389  nan        0.226389  00:09     \n",
      "37        1.735812    3.587609    0.229167  nan        0.229167  00:09     \n",
      "38        1.711161    4.094413    0.145833  nan        0.145833  00:09     \n",
      "39        1.667590    3.523588    0.218056  nan        0.218056  00:09     \n",
      "40        1.624529    3.637499    0.211111  nan        0.211111  00:09     \n",
      "41        1.605170    3.287014    0.251389  0.348994   0.251389  00:09     \n",
      "42        1.583388    4.773452    0.150000  nan        0.150000  00:09     \n",
      "43        1.573755    3.618552    0.190278  nan        0.190278  00:09     \n",
      "44        1.539799    3.147765    0.288889  nan        0.288889  00:09     \n",
      "45        1.501002    3.255687    0.244444  nan        0.244444  00:09     \n",
      "46        1.495002    3.303663    0.229167  nan        0.229167  00:09     \n",
      "47        1.496013    3.727254    0.211111  nan        0.211111  00:09     \n",
      "48        1.461251    3.462981    0.244444  0.305924   0.244444  00:09     \n",
      "49        1.426543    3.497763    0.259722  nan        0.259722  00:09     \n",
      "50        1.439668    3.385573    0.255556  nan        0.255556  00:09     \n",
      "51        1.401990    3.149036    0.273611  nan        0.273611  00:09     \n",
      "52        1.380890    3.315200    0.230556  nan        0.230556  00:09     \n",
      "53        1.395618    3.265530    0.240278  nan        0.240278  00:09     \n",
      "54        1.342689    2.974734    0.284722  0.339288   0.284722  00:09     \n",
      "55        1.371872    2.988655    0.283333  0.323226   0.283333  00:09     \n",
      "56        1.330782    3.155228    0.269444  nan        0.269444  00:09     \n",
      "57        1.327454    3.130638    0.258333  0.312441   0.258333  00:09     \n",
      "58        1.306269    3.082966    0.261111  0.304402   0.261111  00:09     \n",
      "59        1.288323    2.991269    0.283333  0.321177   0.283333  00:09     \n",
      "60        1.274601    3.094887    0.262500  0.318040   0.262500  00:09     \n",
      "61        1.266368    3.137547    0.254167  nan        0.254167  00:09     \n",
      "62        1.262864    2.947692    0.302778  0.328314   0.302778  00:09     \n",
      "63        1.260477    3.557723    0.241667  nan        0.241667  00:09     \n",
      "64        1.256526    3.091158    0.279167  0.338009   0.279167  00:09     \n",
      "65        1.217894    3.097811    0.261111  0.305836   0.261111  00:09     \n",
      "66        1.212165    3.039525    0.280556  0.317097   0.280556  00:09     \n",
      "67        1.230707    3.092311    0.255556  0.319064   0.255556  00:09     \n",
      "68        1.197486    3.083095    0.268056  nan        0.268056  00:09     \n",
      "69        1.196204    3.372018    0.220833  nan        0.220833  00:09     \n",
      "70        1.146708    3.090134    0.252778  nan        0.252778  00:09     \n",
      "71        1.196257    3.068125    0.273611  0.317353   0.273611  00:09     \n",
      "72        1.151867    3.123129    0.269444  nan        0.269444  00:09     \n",
      "73        1.104908    3.095507    0.256944  0.316792   0.256944  00:09     \n",
      "74        1.130903    2.990499    0.281944  0.348522   0.281945  00:09     \n",
      "75        1.134755    2.918094    0.280556  0.297359   0.280556  00:09     \n",
      "76        1.150430    2.906255    0.297222  0.320191   0.297222  00:09     \n",
      "77        1.138083    3.061353    0.261111  0.320371   0.261111  00:09     \n",
      "78        1.102547    3.113709    0.266667  0.329785   0.266667  00:09     \n",
      "79        1.110426    2.893249    0.294444  0.324012   0.294445  00:09     \n",
      "80        1.126131    3.403898    0.236111  nan        0.236111  00:09     \n",
      "81        1.083952    2.922667    0.304167  0.328130   0.304167  00:09     \n",
      "82        1.086648    3.023793    0.290278  0.331646   0.290278  00:09     \n",
      "83        1.057109    2.916461    0.297222  0.320133   0.297222  00:09     \n",
      "84        1.078157    2.926962    0.298611  0.334374   0.298611  00:09     \n",
      "85        1.043809    2.935998    0.291667  0.317552   0.291667  00:09     \n",
      "86        1.033452    2.922821    0.300000  0.329659   0.300000  00:09     \n",
      "87        1.081724    2.931464    0.306944  0.330068   0.306945  00:09     \n",
      "88        1.057350    2.922723    0.295833  0.319326   0.295833  00:09     \n",
      "89        1.057559    2.930747    0.288889  0.316059   0.288889  00:09     \n",
      "90        1.040873    2.916707    0.304167  0.333253   0.304167  00:09     \n",
      "91        1.039363    2.917132    0.301389  0.315462   0.301389  00:09     \n",
      "92        1.054401    2.899961    0.300000  0.317395   0.300000  00:09     \n",
      "93        1.046262    2.906647    0.298611  0.323919   0.298611  00:09     \n",
      "94        1.029184    2.913402    0.293056  0.309836   0.293056  00:09     \n",
      "95        1.041264    2.918845    0.297222  0.327580   0.297222  00:09     \n",
      "96        1.042521    2.919867    0.301389  0.317806   0.301389  00:09     \n",
      "97        1.025003    2.921092    0.295833  0.315536   0.295833  00:09     \n",
      "98        1.024885    2.922163    0.293056  0.320800   0.293056  00:09     \n",
      "99        1.032652    2.917522    0.293056  0.317282   0.293056  00:09     \n",
      "model=resnet18, pretrained=False, mixup=False\n",
      "epoch     train_loss  valid_loss  accuracy  precision  recall    time    \n",
      "0         5.435008    5.066852    0.031944  nan        0.031944  00:09     \n",
      "1         5.057474    4.245301    0.054167  nan        0.054167  00:09     \n",
      "2         4.735806    4.277534    0.050000  nan        0.050000  00:09     \n",
      "3         4.567083    8.795753    0.050000  nan        0.050000  00:09     \n",
      "4         4.410488    4.302146    0.063889  nan        0.063889  00:09     \n",
      "5         4.300265    9.749680    0.056944  nan        0.056944  00:09     \n",
      "6         4.179840    4.087058    0.063889  nan        0.063889  00:09     \n",
      "7         4.036100    33.093399   0.020833  nan        0.020833  00:09     \n",
      "8         3.911688    5.044472    0.029167  nan        0.029167  00:09     \n",
      "9         3.732339    4.539086    0.088889  nan        0.088889  00:09     \n",
      "10        3.623662    5.264975    0.065278  nan        0.065278  00:09     \n",
      "11        3.265013    7.377069    0.052778  nan        0.052778  00:09     \n",
      "12        2.987892    6.768812    0.043056  nan        0.043056  00:09     \n",
      "13        2.736858    3.984450    0.134722  nan        0.134722  00:09     \n",
      "14        2.595177    3.906261    0.141667  nan        0.141667  00:09     \n",
      "15        2.428432    3.504416    0.186111  nan        0.186111  00:09     \n",
      "16        2.298930    3.230226    0.202778  nan        0.202778  00:09     \n",
      "17        2.162562    3.393076    0.220833  nan        0.220833  00:09     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18        1.934137    4.176290    0.179167  nan        0.179167  00:09     \n",
      "19        1.862199    4.246413    0.144444  nan        0.144444  00:09     \n",
      "20        1.644065    3.292498    0.245833  nan        0.245833  00:09     \n",
      "21        1.578485    4.729488    0.175000  nan        0.175000  00:09     \n",
      "22        1.417178    3.623028    0.261111  nan        0.261111  00:09     \n",
      "23        1.332195    5.077959    0.145833  nan        0.145833  00:09     \n",
      "24        1.179957    4.208917    0.194444  nan        0.194444  00:09     \n",
      "25        1.106235    4.018519    0.233333  nan        0.233333  00:09     \n",
      "26        1.015506    4.034355    0.251389  nan        0.251389  00:09     \n",
      "27        0.894108    3.964525    0.277778  nan        0.277778  00:09     \n",
      "28        0.867856    4.558999    0.226389  nan        0.226389  00:09     \n",
      "29        0.851641    5.044768    0.194444  nan        0.194444  00:09     \n",
      "30        0.755077    4.301697    0.259722  nan        0.259722  00:09     \n",
      "31        0.671699    4.914693    0.220833  nan        0.220833  00:09     \n",
      "32        0.695679    4.484950    0.223611  nan        0.223611  00:09     \n",
      "33        0.611203    4.426856    0.256944  0.339579   0.256945  00:09     \n",
      "34        0.540964    4.602781    0.256944  nan        0.256944  00:09     \n",
      "35        0.527543    4.748334    0.245833  nan        0.245833  00:09     \n",
      "36        0.510809    4.752874    0.263889  0.324111   0.263889  00:09     \n",
      "37        0.534328    4.437821    0.255556  0.301730   0.255556  00:09     \n",
      "38        0.506898    5.010839    0.243056  nan        0.243056  00:09     \n",
      "39        0.388446    6.476583    0.201389  0.342076   0.201389  00:09     \n",
      "40        0.392518    5.323013    0.218056  nan        0.218056  00:09     \n",
      "41        0.390724    5.010503    0.265278  0.301201   0.265278  00:09     \n",
      "42        0.343058    5.510877    0.247222  nan        0.247222  00:09     \n",
      "43        0.314389    5.536749    0.277778  nan        0.277778  00:09     \n",
      "44        0.292091    6.571432    0.212500  0.284630   0.212500  00:09     \n",
      "45        0.328429    6.371779    0.213889  nan        0.213889  00:09     \n",
      "46        0.271913    5.647336    0.266667  nan        0.266667  00:09     \n",
      "47        0.284259    5.246776    0.287500  nan        0.287500  00:09     \n",
      "48        0.259959    5.435660    0.243056  0.325012   0.243056  00:09     \n",
      "49        0.285045    5.497797    0.279167  0.314276   0.279167  00:09     \n",
      "50        0.233231    5.933803    0.266667  nan        0.266667  00:09     \n",
      "51        0.197931    5.426313    0.284722  0.324583   0.284722  00:09     \n",
      "52        0.209867    5.570847    0.276389  nan        0.276389  00:09     \n",
      "53        0.186849    6.200962    0.240278  nan        0.240278  00:09     \n",
      "54        0.201669    5.495102    0.284722  0.341229   0.284722  00:09     \n",
      "55        0.195427    5.477986    0.290278  nan        0.290278  00:09     \n",
      "56        0.134740    5.427017    0.304167  nan        0.304167  00:09     \n",
      "57        0.147739    6.579959    0.229167  nan        0.229167  00:09     \n",
      "58        0.171422    5.880361    0.322222  0.400571   0.322222  00:09     \n",
      "59        0.157602    5.566743    0.300000  0.320216   0.300000  00:09     \n",
      "60        0.112395    6.002497    0.294444  0.359177   0.294444  00:09     \n",
      "61        0.085494    6.140641    0.270833  0.320153   0.270833  00:09     \n",
      "62        0.092147    6.581141    0.266667  0.338129   0.266667  00:09     \n",
      "63        0.119329    6.763675    0.262500  nan        0.262500  00:09     \n",
      "64        0.103983    6.460585    0.277778  0.337768   0.277778  00:09     \n",
      "65        0.078165    5.828171    0.295833  0.323564   0.295833  00:09     \n",
      "66        0.071675    6.057703    0.313889  0.365883   0.313889  00:09     \n",
      "67        0.052578    6.047603    0.288889  0.342610   0.288889  00:09     \n",
      "68        0.040333    5.718976    0.302778  0.320119   0.302778  00:09     \n",
      "69        0.045252    5.746191    0.306944  0.327568   0.306945  00:09     \n",
      "70        0.056113    6.112386    0.302778  0.327521   0.302778  00:09     \n",
      "71        0.044718    5.799591    0.322222  0.350409   0.322222  00:09     \n",
      "72        0.027513    5.801498    0.312500  0.342414   0.312500  00:09     \n",
      "73        0.037588    5.824871    0.323611  0.335770   0.323611  00:09     \n",
      "74        0.021620    6.117341    0.304167  0.353340   0.304167  00:09     \n",
      "75        0.031075    5.707651    0.313889  0.348399   0.313889  00:09     \n",
      "76        0.021222    5.786992    0.313889  0.339634   0.313889  00:09     \n",
      "77        0.020326    5.650686    0.320833  0.343971   0.320833  00:09     \n",
      "78        0.018920    5.814524    0.309722  0.341882   0.309722  00:09     \n",
      "79        0.014957    5.760980    0.336111  0.373214   0.336111  00:09     \n",
      "80        0.012129    5.600533    0.341667  0.367172   0.341667  00:09     \n",
      "81        0.010511    5.467418    0.331944  0.347978   0.331944  00:09     \n",
      "82        0.012628    5.491236    0.327778  0.355883   0.327778  00:09     \n",
      "83        0.011866    5.766931    0.322222  0.360960   0.322222  00:09     \n",
      "84        0.006371    5.646857    0.329167  0.349576   0.329167  00:09     \n",
      "85        0.005310    5.539330    0.337500  0.363447   0.337500  00:09     \n",
      "86        0.004224    5.707695    0.330556  0.352379   0.330556  00:09     \n",
      "87        0.007282    5.474973    0.337500  0.362396   0.337500  00:09     \n",
      "88        0.003751    5.610216    0.329167  0.360660   0.329167  00:09     \n",
      "89        0.004866    5.509483    0.343056  0.365012   0.343056  00:09     \n",
      "90        0.002810    5.400989    0.331944  0.358313   0.331944  00:09     \n",
      "91        0.003572    5.343902    0.340278  0.364048   0.340278  00:09     \n",
      "92        0.003402    5.350820    0.329167  0.350968   0.329167  00:09     \n",
      "93        0.002196    5.356989    0.340278  0.359082   0.340278  00:09     \n",
      "94        0.002266    5.339980    0.344444  0.364140   0.344444  00:09     \n",
      "95        0.004380    5.387363    0.348611  0.371387   0.348611  00:09     \n",
      "96        0.002200    5.375118    0.336111  0.359791   0.336111  00:09     \n",
      "97        0.002218    5.371857    0.333333  0.345774   0.333333  00:09     \n",
      "98        0.001803    5.347816    0.347222  0.372189   0.347222  00:09     \n",
      "99        0.003898    5.356966    0.338889  0.356719   0.338889  00:09     \n",
      "model=resnet34, pretrained=True, mixup=True\n",
      "epoch     train_loss  valid_loss  accuracy  precision  recall    time    \n",
      "0         5.763346    4.614819    0.029167  nan        0.029167  00:11     \n",
      "1         5.231652    4.416332    0.041667  nan        0.041667  00:11     \n",
      "2         4.801183    4.324799    0.052778  nan        0.052778  00:11     \n",
      "3         4.528257    4.260694    0.048611  nan        0.048611  00:11     \n",
      "4         4.269639    4.190224    0.059722  nan        0.059722  00:11     \n",
      "5         4.109149    4.111603    0.061111  nan        0.061111  00:11     \n",
      "6         3.966861    4.143390    0.062500  nan        0.062500  00:11     \n",
      "7         3.883213    4.060204    0.075000  nan        0.075000  00:11     \n",
      "8         3.755226    4.041169    0.080556  nan        0.080556  00:11     \n",
      "9         3.720878    4.102704    0.086111  nan        0.086111  00:11     \n",
      "10        3.661637    4.126010    0.080556  nan        0.080556  00:11     \n",
      "11        3.629271    4.041759    0.080556  nan        0.080556  00:11     \n",
      "12        3.586669    3.895780    0.108333  nan        0.108333  00:11     \n",
      "13        3.544885    3.893796    0.122222  nan        0.122222  00:11     \n",
      "14        3.517119    3.817899    0.137500  nan        0.137500  00:11     \n",
      "15        3.407300    3.783010    0.120833  nan        0.120833  00:11     \n",
      "16        3.298735    3.766748    0.144444  nan        0.144444  00:11     \n",
      "17        3.233883    3.732713    0.133333  nan        0.133333  00:11     \n",
      "18        3.182504    3.443944    0.170833  nan        0.170833  00:11     \n",
      "19        3.094836    3.381096    0.193056  nan        0.193056  00:11     \n",
      "20        3.069098    3.611281    0.156944  nan        0.156944  00:11     \n",
      "21        3.047339    3.456586    0.187500  nan        0.187500  00:11     \n",
      "22        2.966514    3.325591    0.186111  nan        0.186111  00:11     \n",
      "23        2.943859    3.284865    0.188889  nan        0.188889  00:11     \n",
      "24        2.857257    3.160608    0.225000  nan        0.225000  00:11     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25        2.814758    3.353134    0.177778  nan        0.177778  00:11     \n",
      "26        2.819920    3.182096    0.208333  nan        0.208333  00:11     \n",
      "27        2.735967    3.132349    0.209722  nan        0.209722  00:11     \n",
      "38        1.793312    2.864043    0.358333  nan        0.358333  00:45     \n",
      "39        1.799658    2.626933    0.355556  nan        0.355556  00:45     \n",
      "40        1.799417    2.369328    0.405556  0.458716   0.405556  00:45     \n",
      "41        1.770010    2.586621    0.365278  nan        0.365278  00:45     \n",
      "42        1.749387    2.384364    0.420833  0.535204   0.420833  00:45     \n",
      "43        1.719476    2.462113    0.377778  0.488405   0.377778  00:45     \n",
      "44        1.648453    2.399096    0.397222  0.456076   0.397222  00:45     \n",
      "45        1.708221    2.412323    0.404167  nan        0.404167  00:46     \n",
      "46        1.713530    2.504405    0.375000  0.454383   0.375000  00:46     \n",
      "47        1.647959    2.506886    0.394444  nan        0.394444  00:45     \n",
      "48        1.634604    2.461579    0.379167  0.443159   0.379167  00:45     \n",
      "49        1.613338    2.550172    0.380556  nan        0.380556  00:45     \n",
      "50        1.548351    2.430175    0.394444  0.476846   0.394444  00:45     \n",
      "51        1.583331    2.511656    0.375000  nan        0.375000  00:45     \n",
      "52        1.571667    2.538127    0.370833  0.445145   0.370833  00:45     \n",
      "53        1.529656    2.471228    0.383333  0.466776   0.383333  00:45     \n",
      "54        1.530966    2.530910    0.375000  0.485279   0.375000  00:45     \n",
      "55        1.535300    2.306166    0.427778  0.488905   0.427778  00:45     \n",
      "56        1.511484    2.234346    0.431944  0.490065   0.431945  00:46     \n",
      "57        1.471749    2.322920    0.438889  0.467121   0.438889  00:45     \n",
      "58        1.455851    2.287014    0.416667  0.460528   0.416667  00:45     \n",
      "59        1.467872    2.217701    0.429167  0.480110   0.429167  00:45     \n",
      "60        1.413751    2.272174    0.400000  0.473179   0.400000  00:45     \n",
      "61        1.445274    2.335243    0.422222  0.484952   0.422222  00:45     \n",
      "62        1.446701    2.331338    0.415278  0.470465   0.415278  00:45     \n",
      "63        1.372749    2.246557    0.430556  nan        0.430556  00:45     \n",
      "64        1.346273    2.267369    0.426389  0.474650   0.426389  00:45     \n",
      "65        1.345034    2.280663    0.445833  0.494021   0.445833  00:45     \n",
      "66        1.364236    2.217254    0.430556  nan        0.430556  00:45     \n",
      "67        1.350099    2.425460    0.391667  0.471146   0.391667  00:45     \n",
      "68        1.311514    2.406085    0.395833  nan        0.395833  00:45     \n",
      "69        1.373420    2.280306    0.437500  0.479426   0.437500  00:45     \n",
      "70        1.315673    2.223471    0.443056  0.492058   0.443056  00:45     \n",
      "71        1.308387    2.183427    0.444444  0.484833   0.444444  00:45     \n",
      "72        1.312815    2.251055    0.423611  0.455347   0.423611  00:45     \n",
      "73        1.299425    2.178299    0.450000  0.476465   0.450000  00:45     \n",
      "74        1.298051    2.156779    0.451389  0.495076   0.451389  00:45     \n",
      "75        1.241492    2.236234    0.434722  0.496081   0.434722  00:45     \n",
      "76        1.245952    2.167135    0.450000  0.492098   0.450000  00:45     \n",
      "77        1.250511    2.161809    0.465278  0.498139   0.465278  00:45     \n",
      "78        1.264170    2.178271    0.463889  0.496163   0.463889  00:45     \n",
      "79        1.206606    2.196350    0.463889  0.495481   0.463889  00:45     \n",
      "80        1.226849    2.194986    0.454167  0.498760   0.454167  00:45     \n",
      "81        1.205002    2.175834    0.454167  0.494607   0.454167  00:45     \n",
      "82        1.228667    2.182327    0.454167  0.490493   0.454167  00:45     \n",
      "83        1.228311    2.167359    0.463889  0.506951   0.463889  00:45     \n",
      "84        1.206890    2.170070    0.456944  0.498218   0.456944  00:45     \n",
      "85        1.217203    2.172964    0.458333  0.494293   0.458333  00:45     \n",
      "86        1.250508    2.178648    0.456944  0.486826   0.456945  00:45     \n",
      "87        1.217788    2.159436    0.477778  0.510458   0.477778  00:45     \n",
      "88        1.199284    2.178145    0.473611  0.509323   0.473611  00:45     \n",
      "89        1.206733    2.172122    0.468056  0.499467   0.468056  00:45     \n",
      "90        1.178581    2.170537    0.469444  0.508649   0.469445  00:45     \n",
      "91        1.193954    2.165202    0.463889  0.504202   0.463889  00:45     \n",
      "92        1.202447    2.168528    0.463889  0.502427   0.463889  00:45     \n",
      "93        1.173515    2.160790    0.470833  0.506624   0.470833  00:45     \n",
      "94        1.178316    2.162970    0.468056  0.502273   0.468056  00:45     \n",
      "95        1.206546    2.160121    0.463889  0.500475   0.463889  00:45     \n",
      "96        1.195355    2.168025    0.466667  0.496494   0.466667  00:46     \n",
      "97        1.226025    2.156542    0.472222  0.505226   0.472222  00:45     \n",
      "98        1.178655    2.163277    0.468056  0.499835   0.468056  00:45     \n",
      "99        1.184325    2.168886    0.470833  0.503804   0.470833  00:45     \n",
      "model=densenet161, pretrained=True, mixup=False\n",
      "epoch     train_loss  valid_loss  accuracy  precision  recall    time    \n",
      "0         5.352291    4.341319    0.038889  nan        0.038889  00:46     \n",
      "1         4.185828    4.182747    0.073611  nan        0.073611  00:46     \n",
      "2         3.193041    4.246653    0.070833  nan        0.070833  00:46     \n",
      "3         2.587926    4.544638    0.068056  nan        0.068056  00:46     \n",
      "4         2.025232    4.389410    0.088889  0.072073   0.088889  00:46     \n",
      "5         1.761148    4.675051    0.068056  nan        0.068056  00:46     \n",
      "6         1.843393    4.963913    0.086111  nan        0.086111  00:46     \n",
      "7         1.895531    4.789951    0.079167  nan        0.079167  00:46     \n",
      "8         2.073204    4.889459    0.076389  nan        0.076389  00:46     \n",
      "9         2.283775    4.744217    0.112500  nan        0.112500  00:46     \n",
      "10        2.410798    4.609534    0.111111  nan        0.111111  00:46     \n",
      "11        2.332815    4.727496    0.104167  nan        0.104167  00:46     \n",
      "12        2.396894    4.494570    0.152778  nan        0.152778  00:46     \n",
      "13        2.390817    5.780783    0.094444  nan        0.094444  00:46     \n",
      "14        2.392796    4.203300    0.176389  nan        0.176389  00:46     \n",
      "15        2.123994    5.059945    0.140278  nan        0.140278  00:45     \n",
      "16        2.077055    3.822706    0.194444  nan        0.194444  00:46     \n",
      "17        2.040529    4.456270    0.173611  nan        0.173611  00:46     \n",
      "18        1.818009    3.972332    0.190278  nan        0.190278  00:46     \n",
      "19        1.710125    3.971549    0.216667  nan        0.216667  00:45     \n",
      "20        1.614848    4.332517    0.209722  nan        0.209722  00:46     \n",
      "21        1.536497    4.157498    0.198611  nan        0.198611  00:45     \n",
      "22        1.439629    3.504868    0.258333  nan        0.258333  00:45     \n",
      "23        1.291034    3.846255    0.258333  nan        0.258333  00:46     \n",
      "24        1.324298    3.760813    0.252778  nan        0.252778  00:45     \n",
      "25        1.265078    3.235075    0.297222  nan        0.297222  00:45     \n",
      "26        1.141730    3.447499    0.286111  nan        0.286111  00:45     \n",
      "27        1.074088    4.028122    0.261111  nan        0.261111  00:46     \n",
      "28        1.023861    3.837614    0.290278  nan        0.290278  00:45     \n",
      "29        1.011201    3.841577    0.258333  nan        0.258333  00:45     \n",
      "30        0.975707    3.803384    0.286111  nan        0.286111  00:45     \n",
      "31        0.867541    4.468575    0.256944  nan        0.256944  00:45     \n",
      "32        0.727855    3.819325    0.302778  nan        0.302778  00:45     \n",
      "33        0.824929    3.868096    0.311111  nan        0.311111  00:45     \n",
      "34        0.712477    3.820027    0.320833  nan        0.320833  00:45     \n",
      "35        0.771641    3.437533    0.336111  nan        0.336111  00:45     \n",
      "36        0.679323    3.389302    0.384722  nan        0.384722  00:45     \n",
      "37        0.607970    3.703478    0.359722  nan        0.359722  00:45     \n",
      "38        0.630349    4.286004    0.306944  nan        0.306944  00:45     \n",
      "39        0.554575    4.370997    0.311111  0.462425   0.311111  00:45     \n",
      "40        0.562238    4.102482    0.311111  nan        0.311111  00:45     \n",
      "41        0.515660    5.019742    0.270833  nan        0.270833  00:45     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42        0.521083    4.145474    0.327778  nan        0.327778  00:45     \n",
      "43        0.463152    3.810530    0.343056  0.433478   0.343056  00:45     \n",
      "44        0.457513    3.936342    0.362500  0.420911   0.362500  00:45     \n",
      "45        0.424602    4.299943    0.352778  nan        0.352778  00:45     \n",
      "46        0.412185    4.079800    0.370833  0.445607   0.370833  00:45     \n",
      "47        0.416420    4.110639    0.384722  nan        0.384722  00:45     \n",
      "48        0.378118    4.063767    0.379167  0.469577   0.379167  00:45     \n",
      "49        0.365915    4.094425    0.373611  0.456728   0.373611  00:45     \n",
      "50        0.317210    3.853395    0.391667  0.429020   0.391667  00:45     \n",
      "51        0.317054    3.835014    0.370833  0.423635   0.370833  00:45     \n",
      "52        0.301419    3.778551    0.383333  nan        0.383333  00:45     \n",
      "53        0.268808    5.036678    0.329167  nan        0.329167  00:45     \n",
      "54        0.263499    4.514032    0.362500  0.462826   0.362500  00:45     \n",
      "55        0.286943    3.908133    0.390278  0.476797   0.390278  00:45     \n",
      "56        0.233136    4.100869    0.388889  0.444543   0.388889  00:45     \n",
      "57        0.232089    4.310504    0.376389  nan        0.376389  00:45     \n",
      "58        0.214639    4.157371    0.386111  0.467059   0.386111  00:45     \n",
      "59        0.164022    4.179740    0.402778  nan        0.402778  00:45     \n",
      "60        0.181805    4.544314    0.368056  0.451051   0.368056  00:45     \n",
      "61        0.169047    4.212094    0.401389  0.457937   0.401389  00:45     \n",
      "62        0.154461    4.091853    0.387500  0.420055   0.387500  00:45     \n",
      "63        0.117154    4.320811    0.384722  0.440769   0.384722  00:45     \n",
      "64        0.139073    4.269946    0.384722  0.434825   0.384722  00:45     \n",
      "65        0.132238    4.025043    0.420833  0.465759   0.420833  00:45     \n",
      "66        0.127120    4.021898    0.418056  0.461891   0.418056  00:45     \n",
      "67        0.121150    4.187002    0.408333  0.450280   0.408333  00:45     \n",
      "68        0.092558    4.098555    0.405556  0.445873   0.405556  00:45     \n",
      "69        0.094588    4.225543    0.393056  0.433934   0.393056  00:45     \n",
      "70        0.085467    4.039056    0.409722  0.453502   0.409722  00:45     \n",
      "71        0.064124    4.086545    0.405556  0.460617   0.405556  00:45     \n",
      "72        0.059875    4.219321    0.405556  0.433314   0.405556  00:45     \n",
      "73        0.060567    4.108068    0.390278  0.445787   0.390278  00:45     \n",
      "74        0.040481    4.018008    0.416667  0.442936   0.416667  00:45     \n",
      "75        0.045430    4.115088    0.397222  0.423006   0.397222  00:45     \n",
      "76        0.050306    4.112144    0.419444  0.444856   0.419444  00:45     \n",
      "77        0.044011    4.023936    0.412500  0.436477   0.412500  00:45     \n",
      "78        0.038463    4.014001    0.434722  0.470450   0.434722  00:45     \n",
      "79        0.037745    3.935000    0.438889  0.466172   0.438889  00:45     \n",
      "80        0.033223    3.865173    0.437500  0.458664   0.437500  00:45     \n",
      "81        0.025101    3.784440    0.425000  0.450522   0.425000  00:45     \n",
      "82        0.018044    3.837320    0.441667  0.468014   0.441667  00:45     \n",
      "83        0.020770    3.880507    0.440278  0.468492   0.440278  00:45     \n",
      "84        0.012335    3.872395    0.448611  0.474700   0.448611  00:45     \n",
      "85        0.015762    3.892687    0.436111  0.461728   0.436111  00:45     \n",
      "86        0.013573    3.838295    0.443056  0.480101   0.443056  00:45     \n",
      "87        0.012565    3.873957    0.441667  0.465439   0.441667  00:45     \n",
      "88        0.013489    3.868772    0.427778  0.450950   0.427778  00:45     \n",
      "89        0.010943    3.848822    0.443056  0.468908   0.443056  00:45     \n",
      "90        0.012068    3.783723    0.450000  0.475383   0.450000  00:45     \n",
      "91        0.010294    3.843673    0.440278  0.450705   0.440278  00:45     \n",
      "92        0.010864    3.828347    0.441667  0.469350   0.441667  00:45     \n",
      "93        0.007004    3.802925    0.438889  0.457565   0.438889  00:45     \n",
      "94        0.008286    3.818694    0.448611  0.465619   0.448611  00:45     \n",
      "95        0.008992    3.822784    0.441667  0.459243   0.441667  00:45     \n",
      "96        0.007512    3.835461    0.437500  0.458449   0.437500  00:45     \n",
      "97        0.006856    3.875386    0.447222  0.468708   0.447222  00:45     \n",
      "98        0.006888    3.811989    0.441667  0.466290   0.441667  00:45     \n",
      "99        0.006251    3.804549    0.447222  0.474138   0.447222  00:45     \n",
      "model=densenet161, pretrained=False, mixup=True\n",
      "epoch     train_loss  valid_loss  accuracy  precision  recall    time    \n",
      "█\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.79 GiB total capacity; 5.63 GiB already allocated; 5.69 MiB free; 5.76 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5976f7e94dd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Split{valid_split}_{model.__name__},pretrained={int(pretrained)},mixup={int(use_mixup)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minit_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_checkpoint_bottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mbn_function\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# type: (List[Tensor]) -> Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mconcated_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcated_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: T484\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbottleneck_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.79 GiB total capacity; 5.63 GiB already allocated; 5.69 MiB free; 5.76 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for valid_split in VALID_SPLITS:\n",
    "    print(f\"************************************\\nSPLIT {valid_split}\\n************************************\")\n",
    "    for model in MODELS:\n",
    "        for pretrained in PRETRAINED:\n",
    "            for use_mixup in USE_MIXUP:\n",
    "                # Free up GPU memory\n",
    "                print(f\"model={model.__name__}, pretrained={pretrained}, mixup={use_mixup}\")\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                learn = cnn_learner(get_data(valid_split, bs=32),\n",
    "                                    model,\n",
    "                                    pretrained=pretrained,\n",
    "                                    metrics=[accuracy, metrics.Precision(average=\"macro\"), metrics.Recall(average=\"macro\")],\n",
    "                                    # Using macro average precision and recall\n",
    "                                    callback_fns=[ShowGraph,\n",
    "                                                  partial(callbacks.CSVLogger, filename=f\"{model.__name__},pretrained={int(pretrained)},mixup={int(use_mixup)}\")\n",
    "                                                 ]).to_fp16()\n",
    "                if use_mixup:\n",
    "                    learn = learn.mixup()\n",
    "\n",
    "                learn.fit_one_cycle(100, max_lr=1e-2)\n",
    "                learn.export(f\"Split{valid_split}_{model.__name__},pretrained={int(pretrained)},mixup={int(use_mixup)}\")\n",
    "                del learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
